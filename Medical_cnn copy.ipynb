{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Edited 02/11/2024 \n",
    "Do not Edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageEnhance\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self, image_path):\n",
    "        self.image_path = image_path\n",
    "        self.original_img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    def apply_clahe(self, clip_limit=4.0, tile_grid_size=(1, 1)):\n",
    "        clahe = cv2.createCLAHE(clipLimit=clip_limit, tileGridSize=tile_grid_size)\n",
    "        clahe_img = clahe.apply(self.original_img)\n",
    "        return clahe_img\n",
    "\n",
    "    def gamma_correction(self, image, gamma=0.9):\n",
    "        inv_gamma = 1.0 / gamma\n",
    "        gamma_img = np.array(255 * (image / 255) ** inv_gamma, dtype='uint8')\n",
    "        return gamma_img\n",
    "\n",
    "    def unsharp_mask(self, image, strength=1.5, blur_size=(3, 3)):\n",
    "        blurred = cv2.GaussianBlur(image, blur_size, 0)\n",
    "        sharpened_img = cv2.addWeighted(image, 1 + strength, blurred, -strength, 0)\n",
    "        return sharpened_img\n",
    "\n",
    "    def gaussian_smoothing(self, image, kernel_size=(3, 3)):\n",
    "        smoothed_img = cv2.GaussianBlur(image, kernel_size, 0)\n",
    "        return smoothed_img\n",
    "\n",
    "    def normalize_image(self, image):\n",
    "        normalized_img = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        return normalized_img\n",
    "\n",
    "    def resize_image(self, image, target_size):\n",
    "        resized_img = np.array(Image.fromarray((image * 255).astype(np.uint8)).resize(target_size))\n",
    "        return resized_img\n",
    "\n",
    "    def process_and_display(self, resize=(256, 256)):\n",
    "        # Perform all preprocessing steps\n",
    "        clahe_img = self.apply_clahe()\n",
    "        gamma_img = self.gamma_correction(clahe_img)\n",
    "        sharpened_img = self.unsharp_mask(gamma_img)\n",
    "        smoothed_img = self.gaussian_smoothing(sharpened_img)\n",
    "        normalized_img = self.normalize_image(smoothed_img)\n",
    "        final_resized_img = self.resize_image(normalized_img, resize)\n",
    "\n",
    "        # Plot the images with headings\n",
    "        images = [self.original_img, clahe_img, gamma_img, sharpened_img, smoothed_img, final_resized_img]\n",
    "        titles = ['Original Image', 'CLAHE', 'Gamma Corrected', 'Unsharp Mask', 'Gaussian Smoothed', 'Final Resized']\n",
    "\n",
    "        plt.figure(figsize=(15, 4))\n",
    "        for i, (img, title) in enumerate(zip(images, titles)):\n",
    "            plt.subplot(1, 6, i + 1)\n",
    "            plt.imshow(img, cmap='gray')\n",
    "            plt.title(title)\n",
    "            plt.axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        return final_resized_img\n",
    "\n",
    "# Usage\n",
    "image_path = 'DATASETS/split_data/train/0/9996086L.png'\n",
    "preprocessor = ImagePreprocessor(image_path)\n",
    "final_image = preprocessor.process_and_display((512, 512))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "def process_dataset(dataset_dir):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Count total files for progress bar\n",
    "    total_files = sum(len(files) for _, _, files in os.walk(dataset_dir))\n",
    "\n",
    "    with tqdm(total=total_files, unit=\"images\", desc=\"Preprocessing dataset\") as pbar:\n",
    "        for dirname, _, filenames in os.walk(dataset_dir):\n",
    "            \n",
    "            # Check if directory name is numeric, indicating a label folder\n",
    "            if os.path.basename(dirname).isdigit():\n",
    "                label = int(os.path.basename(dirname))  # Folder name as the label\n",
    "\n",
    "                for filename in filenames:\n",
    "                    image_path = os.path.join(dirname, filename)\n",
    "                    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "                    # Initialize preprocessor with the image path\n",
    "                    preprocessor = ImagePreprocessor(image_path)\n",
    "                    clahe_img = preprocessor.apply_clahe()\n",
    "                    gamma_img = preprocessor.gamma_correction(clahe_img)\n",
    "                    sharpened_img = preprocessor.unsharp_mask(gamma_img)\n",
    "                    smoothed_img = preprocessor.gaussian_smoothing(sharpened_img)\n",
    "                    normalized_img = preprocessor.normalize_image(smoothed_img)\n",
    "                    final_resized_img = preprocessor.resize_image(normalized_img, (512, 512))\n",
    "\n",
    "                    X.append(final_resized_img)\n",
    "                    y.append(label)\n",
    "\n",
    "                    pbar.update(1)\n",
    "        # Convert lists to NumPy arrays first\n",
    "        X_array = np.array(X)  # Convert to NumPy array\n",
    "        y_array = np.array(y)\n",
    "        # Convert lists to PyTorch tensors\n",
    "        X_tensor = torch.tensor(X_array).float()  # Convert to float tensor\n",
    "        y_tensor = torch.tensor(y_array).long()   # Convert to long tensor for labels\n",
    "    return X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "dataset_dir =[\"DATASETS/split_data/train/\"]#\"DATASETS/split_data/train/\",\"DATASETS/train/train/\"\n",
    "X_train, y_train = None, None\n",
    "\n",
    "for data in dataset_dir:\n",
    "    X_temp, y_temp = process_dataset(data)\n",
    "    if X_train is None:\n",
    "        X_train, y_train = X_temp, y_temp\n",
    "    X_train=torch.cat((X_train, X_temp), dim=0)\n",
    "    y_train=torch.cat((y_train, y_temp), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Usage\n",
    "# dataset_dir = \"DATASETS/kneeKL224/train/\"\n",
    "# X_train1, y_train1 = process_dataset(dataset_dir)\n",
    "# # Usage\n",
    "# dataset_dir2 = \"DATASETS/kneeKL299/train/\"\n",
    "# X_train2, y_train2 = process_dataset(dataset_dir2)\n",
    "# # Usage\n",
    "# dataset_dir3 = \"DATASETS/split_data/train/\"\n",
    "# X_train3, y_train3 = process_dataset(dataset_dir3)\n",
    "# # Usage\n",
    "# dataset_dir4 = \"DATASETS/train/train/\"\n",
    "# X_train4, y_train4 = process_dataset(dataset_dir4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Combine along a specific dimension (e.g., dimension 0)\n",
    "# X_train = torch.cat((X_train1, X_train2,X_train3,X_train4), dim=0)\n",
    "# y_train = torch.cat((y_train1, y_train2,y_train3,y_train4), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"DATASETS/split_data/val/\"\n",
    "X_val, y_val = process_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"DATASETS/split_data/test/\"\n",
    "X_test, y_test = process_dataset(dataset_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot random images with labels to check preprocessing\n",
    "num_samples = 5\n",
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(num_samples):\n",
    "    idx = np.random.randint(0, len(X_train))\n",
    "    plt.subplot(1, num_samples, i + 1)\n",
    "    plt.imshow(X_train[idx], cmap='gray')\n",
    "    plt.title(f\"Label: {y_train[idx]}\")\n",
    "    plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "# Convert the PyTorch tensor to a NumPy array\n",
    "y_train_np = y_train.numpy()\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = collections.Counter(y_train_np)\n",
    "\n",
    "# Print the label distribution\n",
    "print(\"Label distribution:\", label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [label_counts[i] for i in range(len(label_counts))]\n",
    "\n",
    "# Print the label list\n",
    "print(\"Label counts as list:\", label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from torchsummary import torchsummary\n",
    "# import numpy as np\n",
    "\n",
    "# class LightweightMedicalCNN(nn.Module):\n",
    "#     def __init__(self, num_classes, in_channels=1):\n",
    "#         super(LightweightMedicalCNN, self).__init__()\n",
    "        \n",
    "#         # Initial parameters\n",
    "#         self.in_channels = in_channels\n",
    "#         self.num_classes = num_classes\n",
    "        \n",
    "#         # First Convolutional Block\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, 32, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "        \n",
    "#         # Second Convolutional Block with Primary Caps inspiration\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "        \n",
    "#         # Third Convolutional Block\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool2d(kernel_size=2)\n",
    "#         )\n",
    "        \n",
    "#         # Spatial Attention Module\n",
    "#         self.spatial_attention = nn.Sequential(\n",
    "#             nn.Conv2d(128, 1, kernel_size=7, padding=3),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "        \n",
    "#         # Dynamic Routing inspired module\n",
    "#         self.caps_layer = nn.Sequential(\n",
    "#             nn.Conv2d(128, 16, kernel_size=3, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.AdaptiveAvgPool2d((4, 4))\n",
    "#         )\n",
    "        \n",
    "#         # Calculate the size for the flatten layer\n",
    "#         self._to_linear = 16 * 4 * 4\n",
    "        \n",
    "#         # Classification layers\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(self._to_linear, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.Linear(256, num_classes)\n",
    "#         )\n",
    "        \n",
    "#         # Initialize weights\n",
    "#         self._initialize_weights()\n",
    "        \n",
    "#     def _initialize_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.BatchNorm2d):\n",
    "#                 nn.init.constant_(m.weight, 1)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.normal_(m.weight, 0, 0.01)\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         # First block\n",
    "#         x = self.conv1(x)\n",
    "        \n",
    "#         # Second block\n",
    "#         x = self.conv2(x)\n",
    "        \n",
    "#         # Third block\n",
    "#         x = self.conv3(x)\n",
    "        \n",
    "#         # Apply spatial attention\n",
    "#         attention = self.spatial_attention(x)\n",
    "#         x = x * attention\n",
    "        \n",
    "#         # Capsule inspired feature extraction\n",
    "#         x = self.caps_layer(x)\n",
    "        \n",
    "#         # Flatten\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        \n",
    "#         # Classification\n",
    "#         x = self.classifier(x)\n",
    "        \n",
    "#         return x\n",
    "\n",
    "# def print_model_summary(model, input_size=(1, 512, 512)):\n",
    "#     \"\"\"\n",
    "#     Print model summary and calculate model size\n",
    "#     \"\"\"\n",
    "#     # Convert input size to include batch dimension\n",
    "#     batch_size = 1\n",
    "#     input_shape = (batch_size, *input_size)\n",
    "    \n",
    "#     # Create dummy input\n",
    "#     dummy_input = torch.randn(input_shape)\n",
    "    \n",
    "#     # Print model architecture\n",
    "#     print(\"\\nModel Architecture:\")\n",
    "#     print(model)\n",
    "    \n",
    "#     # Calculate total parameters\n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "#     print(f\"\\nTotal Parameters: {total_params:,}\")\n",
    "#     print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    \n",
    "#     # Estimate model size in MB\n",
    "#     model_size_mb = total_params * 4 / (1024 * 1024)  # Assuming 4 bytes per parameter\n",
    "#     print(f\"Estimated Model Size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "#     # Test forward pass\n",
    "#     try:\n",
    "#         output = model(dummy_input)\n",
    "#         print(f\"\\nInput Shape: {input_shape}\")\n",
    "#         print(f\"Output Shape: {output.shape}\")\n",
    "#         print(\"\\nModel summary test passed successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError during forward pass: {str(e)}\")\n",
    "\n",
    "# # Example usage\n",
    "# def test_model(num_classes=5):\n",
    "#     \"\"\"\n",
    "#     Test the model with sample data\n",
    "#     \"\"\"\n",
    "#     # Initialize model\n",
    "#     model = LightweightMedicalCNN(num_classes=num_classes, in_channels=1)\n",
    "    \n",
    "#     # Print model summary\n",
    "#     print_model_summary(model)\n",
    "    \n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple, List\n",
    "\n",
    "class EfficientMedicalNet(nn.Module):\n",
    "    def __init__(self, num_classes: int, in_channels: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Hyperparameters for the network\n",
    "        self.initial_channels = 32\n",
    "        self.reduction_ratio = 4  # For SE blocks\n",
    "        \n",
    "        # Initial convolution with instance norm for better generalization\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, self.initial_channels, kernel_size=7, stride=2, padding=3),\n",
    "            nn.InstanceNorm2d(self.initial_channels, affine=True),  # Added affine=True\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Create the main network structure\n",
    "        self.stage1 = self._make_stage(self.initial_channels, 64, stride=2)\n",
    "        self.stage2 = self._make_stage(64, 128, stride=2)\n",
    "        self.stage3 = self._make_stage(128, 256, stride=2)\n",
    "        self.stage4 = self._make_stage(256, 512, stride=2)\n",
    "        \n",
    "        # Global feature refinement\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "        # Multi-scale feature fusion\n",
    "        self.fusion_conv = nn.Sequential(\n",
    "            nn.Conv2d(960, 512, kernel_size=1),  # 960 = 128 + 256 + 512 + 64\n",
    "            nn.InstanceNorm2d(512, affine=True),  # Added affine=True\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 256),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _make_stage(self, in_channels: int, out_channels: int, stride: int) -> nn.Sequential:\n",
    "        \"\"\"Creates a stage with depthwise separable convolutions and SE attention\"\"\"\n",
    "        return nn.Sequential(\n",
    "            DepthwiseSeparableConv(in_channels, out_channels, stride),\n",
    "            SEBlock(out_channels),\n",
    "            ResidualBlock(out_channels)\n",
    "        )\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights using He initialization\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.InstanceNorm2d) and m.affine:\n",
    "                if m.weight is not None:\n",
    "                    nn.init.ones_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Initial processing\n",
    "        x = self.initial_conv(x)\n",
    "        \n",
    "        # Multi-scale feature extraction\n",
    "        f1 = self.stage1(x)\n",
    "        f2 = self.stage2(f1)\n",
    "        f3 = self.stage3(f2)\n",
    "        f4 = self.stage4(f3)\n",
    "        \n",
    "        # Multi-scale feature fusion\n",
    "        # Upsample all features to f2's size for fusion\n",
    "        f3_up = F.interpolate(f3, size=f2.shape[2:])\n",
    "        f4_up = F.interpolate(f4, size=f2.shape[2:])\n",
    "        f1_down = F.interpolate(f1, size=f2.shape[2:])\n",
    "        \n",
    "        # Concatenate features and apply fusion\n",
    "        fused = torch.cat([f1_down, f2, f3_up, f4_up], dim=1)\n",
    "        fused = self.fusion_conv(fused)\n",
    "        \n",
    "        # Global pooling and classification\n",
    "        out = self.global_pool(fused)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.dropout(out)\n",
    "        out = self.classifier(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            # Depthwise convolution\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, \n",
    "                     padding=1, groups=in_channels, bias=False),\n",
    "            nn.InstanceNorm2d(in_channels, affine=True),\n",
    "            nn.GELU(),\n",
    "            # Pointwise convolution\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        \n",
    "        # Skip connection if dimensions change\n",
    "        self.skip = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True)\n",
    "        ) if stride != 1 or in_channels != out_channels else nn.Identity()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.conv(x) + self.skip(x)\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    def __init__(self, channels: int, reduction: int = 4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "        self.excitation = nn.Sequential(\n",
    "            nn.Linear(channels, channels // reduction),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels // reduction, channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.squeeze(x).view(b, c)\n",
    "        y = self.excitation(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"Residual block with pre-activation\"\"\"\n",
    "    def __init__(self, channels: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.InstanceNorm2d(channels, affine=True),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return x + self.conv(x)\n",
    "\n",
    "def print_model_summary(model: nn.Module, input_size: Tuple[int, int, int] = (1, 512, 512)):\n",
    "    \"\"\"Print model summary and test forward pass\"\"\"\n",
    "    batch_size = 1\n",
    "    input_shape = (batch_size, *input_size)\n",
    "    dummy_input = torch.randn(input_shape)\n",
    "    \n",
    "    # Calculate model statistics\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    model_size_mb = total_params * 4 / (1024 * 1024)\n",
    "    \n",
    "    print(f\"\\nModel Statistics:\")\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "    print(f\"Estimated Model Size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "    try:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            output = model(dummy_input)\n",
    "        print(f\"Input Shape: {dummy_input.shape}\")\n",
    "        print(f\"Output Shape: {output.shape}\")\n",
    "        print(\"\\nModel architecture test passed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError during forward pass: {str(e)}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     model = EfficientMedicalNet(num_classes=5, in_channels=1)\n",
    "#     print_model_summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# from typing import Tuple, List\n",
    "\n",
    "# class LightEfficientMedicalNet(nn.Module):\n",
    "#     def __init__(self, num_classes: int, in_channels: int = 1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         # Reduced initial channels\n",
    "#         self.initial_channels = 16  # Reduced from 32\n",
    "#         self.reduction_ratio = 8    # Increased from 4 for fewer SE parameters\n",
    "        \n",
    "#         # Initial convolution with reduced channels\n",
    "#         self.initial_conv = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, self.initial_channels, kernel_size=5, stride=2, padding=2),  # Reduced kernel size\n",
    "#             nn.InstanceNorm2d(self.initial_channels, affine=True),\n",
    "#             nn.GELU()\n",
    "#         )\n",
    "        \n",
    "#         # Reduced channel progression\n",
    "#         self.stage1 = self._make_stage(self.initial_channels, 32, stride=2)   # Reduced from 64\n",
    "#         self.stage2 = self._make_stage(32, 64, stride=2)                      # Reduced from 128\n",
    "#         self.stage3 = self._make_stage(64, 128, stride=2)                     # Reduced from 256\n",
    "#         self.stage4 = self._make_stage(128, 256, stride=2)                    # Reduced from 512\n",
    "        \n",
    "#         # Global feature refinement\n",
    "#         self.global_pool = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.dropout = nn.Dropout(0.2)\n",
    "        \n",
    "#         # Multi-scale feature fusion with reduced channels\n",
    "#         self.fusion_conv = nn.Sequential(\n",
    "#             nn.Conv2d(480, 256, kernel_size=1),  # 480 = 32 + 64 + 128 + 256\n",
    "#             nn.InstanceNorm2d(256, affine=True),\n",
    "#             nn.GELU()\n",
    "#         )\n",
    "        \n",
    "#         # Classifier with reduced dimensions\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(256, 128),    # Reduced from 512, 256\n",
    "#             nn.GELU(),\n",
    "#             nn.Dropout(0.3),\n",
    "#             nn.Linear(128, num_classes)\n",
    "#         )\n",
    "        \n",
    "#         self._initialize_weights()\n",
    "\n",
    "#     def _make_stage(self, in_channels: int, out_channels: int, stride: int) -> nn.Sequential:\n",
    "#         \"\"\"Creates a lightweight stage\"\"\"\n",
    "#         return nn.Sequential(\n",
    "#             DepthwiseSeparableConv(in_channels, out_channels, stride),\n",
    "#             SEBlock(out_channels, self.reduction_ratio),\n",
    "#             LightResidualBlock(out_channels)  # Using lightweight residual block\n",
    "#         )\n",
    "    \n",
    "#     def _initialize_weights(self):\n",
    "#         for m in self.modules():\n",
    "#             if isinstance(m, nn.Conv2d):\n",
    "#                 nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "#             elif isinstance(m, nn.Linear):\n",
    "#                 nn.init.kaiming_normal_(m.weight)\n",
    "#                 nn.init.zeros_(m.bias)\n",
    "#             elif isinstance(m, nn.InstanceNorm2d) and m.affine:\n",
    "#                 if m.weight is not None:\n",
    "#                     nn.init.ones_(m.weight)\n",
    "#                 if m.bias is not None:\n",
    "#                     nn.init.zeros_(m.bias)\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         x = self.initial_conv(x)\n",
    "        \n",
    "#         f1 = self.stage1(x)\n",
    "#         f2 = self.stage2(f1)\n",
    "#         f3 = self.stage3(f2)\n",
    "#         f4 = self.stage4(f3)\n",
    "        \n",
    "#         f3_up = F.interpolate(f3, size=f2.shape[2:])\n",
    "#         f4_up = F.interpolate(f4, size=f2.shape[2:])\n",
    "#         f1_down = F.interpolate(f1, size=f2.shape[2:])\n",
    "        \n",
    "#         fused = torch.cat([f1_down, f2, f3_up, f4_up], dim=1)\n",
    "#         fused = self.fusion_conv(fused)\n",
    "        \n",
    "#         out = self.global_pool(fused)\n",
    "#         out = out.view(out.size(0), -1)\n",
    "#         out = self.dropout(out)\n",
    "#         out = self.classifier(out)\n",
    "        \n",
    "#         return out\n",
    "\n",
    "# class DepthwiseSeparableConv(nn.Module):\n",
    "#     def __init__(self, in_channels: int, out_channels: int, stride: int = 1):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.conv = nn.Sequential(\n",
    "#             # Depthwise convolution\n",
    "#             nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, \n",
    "#                      padding=1, groups=in_channels, bias=False),\n",
    "#             nn.InstanceNorm2d(in_channels, affine=True),\n",
    "#             nn.GELU(),\n",
    "#             # Pointwise convolution\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, bias=False),\n",
    "#             nn.InstanceNorm2d(out_channels, affine=True),\n",
    "#             nn.GELU()\n",
    "#         )\n",
    "        \n",
    "#         self.skip = nn.Sequential(\n",
    "#             nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "#             nn.InstanceNorm2d(out_channels, affine=True)\n",
    "#         ) if stride != 1 or in_channels != out_channels else nn.Identity()\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         return self.conv(x) + self.skip(x)\n",
    "\n",
    "# class SEBlock(nn.Module):\n",
    "#     \"\"\"Lightweight Squeeze-and-Excitation block\"\"\"\n",
    "#     def __init__(self, channels: int, reduction: int = 8):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.squeeze = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.excitation = nn.Sequential(\n",
    "#             nn.Linear(channels, channels // reduction),\n",
    "#             nn.GELU(),\n",
    "#             nn.Linear(channels // reduction, channels),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         b, c, _, _ = x.size()\n",
    "#         y = self.squeeze(x).view(b, c)\n",
    "#         y = self.excitation(y).view(b, c, 1, 1)\n",
    "#         return x * y\n",
    "\n",
    "# class LightResidualBlock(nn.Module):\n",
    "#     \"\"\"Lightweight residual block with reduced parameters\"\"\"\n",
    "#     def __init__(self, channels: int):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.conv = nn.Sequential(\n",
    "#             nn.InstanceNorm2d(channels, affine=True),\n",
    "#             nn.GELU(),\n",
    "#             # Using a single 3x3 conv instead of two\n",
    "#             nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "#         return x + self.conv(x)\n",
    "\n",
    "# def print_model_summary(model: nn.Module, input_size: Tuple[int, int, int] = (1, 512, 512)):\n",
    "#     batch_size = 1\n",
    "#     input_shape = (batch_size, *input_size)\n",
    "#     dummy_input = torch.randn(input_shape)\n",
    "    \n",
    "#     total_params = sum(p.numel() for p in model.parameters())\n",
    "#     trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "#     model_size_mb = total_params * 4 / (1024 * 1024)\n",
    "    \n",
    "#     print(f\"\\nModel Statistics:\")\n",
    "#     print(f\"Total Parameters: {total_params:,}\")\n",
    "#     print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "#     print(f\"Estimated Model Size: {model_size_mb:.2f} MB\")\n",
    "    \n",
    "#     try:\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             output = model(dummy_input)\n",
    "#         print(f\"Input Shape: {dummy_input.shape}\")\n",
    "#         print(f\"Output Shape: {output.shape}\")\n",
    "#         print(\"\\nModel architecture test passed successfully!\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"\\nError during forward pass: {str(e)}\")\n",
    "\n",
    "# # # Example usage\n",
    "# # if __name__ == \"__main__\":\n",
    "# #     model = LightEfficientMedicalNet(num_classes=5, in_channels=1)\n",
    "# #     print_model_summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do actual Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.amp import autocast, GradScaler\n",
    "import wandb\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, transform=None):\n",
    "        self.X = torch.FloatTensor(X).unsqueeze(1)  # Add channel dimension\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_model(\n",
    "    model: nn.Module,\n",
    "    test_loader: DataLoader,\n",
    "    device: str = None,\n",
    "    save_path: str = None\n",
    "):\n",
    "    \"\"\"Evaluate model on test set and generate detailed metrics\"\"\"\n",
    "    device = device or torch.device(\"cuda\")\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(test_loader, desc='Testing'):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(all_targets, all_preds, output_dict=True)\n",
    "    report_df = pd.DataFrame(report).transpose()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_preds)\n",
    "    plt.figure(figsize=(12,10))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Test Set Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    \n",
    "    if save_path:\n",
    "        # Save results\n",
    "        plt.savefig(os.path.join(save_path, 'confusion_matrix.png'))\n",
    "        report_df.to_csv(os.path.join(save_path, 'classification_report.csv'))\n",
    "    \n",
    "    plt.show()\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report_df)\n",
    "    \n",
    "    return report_df, cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, min_delta=0, mode='min'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, current_val):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = current_val\n",
    "        elif current_val > self.best_loss + self.min_delta and self.mode == 'min':\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = current_val\n",
    "            self.counter = 0\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        config: Dict,\n",
    "        train_loader: DataLoader,\n",
    "        val_loader: DataLoader,\n",
    "        criterion: nn.Module = None,\n",
    "        optimizer: optim.Optimizer = None,\n",
    "        scheduler = None,\n",
    "        device: str = None\n",
    "    ):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion or nn.CrossEntropyLoss()\n",
    "        self.optimizer = optimizer or optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=config['learning_rate'],\n",
    "            weight_decay=config['weight_decay']\n",
    "        )\n",
    "        self.scheduler = scheduler or optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer, mode='min', patience=3, factor=0.1\n",
    "        )\n",
    "        self.device = device or torch.device(\"cuda\")\n",
    "        self.model = self.model.to(self.device)\n",
    "        self.scaler = GradScaler('cuda')\n",
    "        self.early_stopping = EarlyStopping(patience=config['early_stopping_patience'])\n",
    "        \n",
    "        # Initialize metrics tracking\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.best_val_acc = 0.0\n",
    "        \n",
    "        # Setup WandB\n",
    "        self.run = wandb.init(\n",
    "            project=config['project_name'],\n",
    "            config=config,\n",
    "            name=f\"{config['model_name']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        )\n",
    "        \n",
    "        # Save paths\n",
    "        self.save_dir = config['save_dir']\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "        \n",
    "    def train_epoch(self) -> Tuple[float, float]:\n",
    "        self.model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc='Training')\n",
    "        for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "            inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision training\n",
    "            with autocast('cuda'):\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass with gradient scaling\n",
    "            self.scaler.scale(loss).backward()\n",
    "            self.scaler.step(self.optimizer)\n",
    "            self.scaler.update()\n",
    "            \n",
    "            # Metrics\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'loss': running_loss/(batch_idx+1),\n",
    "                'acc': 100.*correct/total\n",
    "            })\n",
    "            \n",
    "        epoch_loss = running_loss / len(self.train_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def validate_epoch(self) -> Tuple[float, float]:\n",
    "        self.model.eval()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(self.val_loader, desc='Validation')\n",
    "            for batch_idx, (inputs, targets) in enumerate(pbar):\n",
    "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
    "                \n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += targets.size(0)\n",
    "                correct += predicted.eq(targets).sum().item()\n",
    "                \n",
    "                all_preds.extend(predicted.cpu().numpy())\n",
    "                all_targets.extend(targets.cpu().numpy())\n",
    "                \n",
    "                pbar.set_postfix({\n",
    "                    'loss': running_loss/(batch_idx+1),\n",
    "                    'acc': 100.*correct/total\n",
    "                })\n",
    "        \n",
    "        epoch_loss = running_loss / len(self.val_loader)\n",
    "        epoch_acc = 100. * correct / total\n",
    "        \n",
    "        # Log confusion matrix to WandB\n",
    "        cm = confusion_matrix(all_targets, all_preds)\n",
    "        plt.figure(figsize=(10,8))\n",
    "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.title('Validation Confusion Matrix')\n",
    "        plt.ylabel('True Label')\n",
    "        plt.xlabel('Predicted Label')\n",
    "        wandb.log({\"confusion_matrix\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "        \n",
    "        return epoch_loss, epoch_acc\n",
    "    \n",
    "    def train(self, epochs: int):\n",
    "        for epoch in range(epochs):\n",
    "            print(f'\\nEpoch {epoch+1}/{epochs}')\n",
    "            \n",
    "            # Training phase\n",
    "            train_loss, train_acc = self.train_epoch()\n",
    "            \n",
    "            # Validation phase\n",
    "            val_loss, val_acc = self.validate_epoch()\n",
    "            \n",
    "            # Learning rate scheduling\n",
    "            self.scheduler.step(val_loss)\n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            # Logging\n",
    "            wandb.log({\n",
    "                \"train_loss\": train_loss,\n",
    "                \"train_acc\": train_acc,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"val_acc\": val_acc,\n",
    "                \"learning_rate\": current_lr\n",
    "            })\n",
    "            \n",
    "            # Save best model\n",
    "            if val_acc > self.best_val_acc:\n",
    "                self.best_val_acc = val_acc\n",
    "                self.save_model('best_model.pth')\n",
    "            \n",
    "            # Early stopping check\n",
    "            self.early_stopping(val_loss)\n",
    "            if self.early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "        \n",
    "        # Save final model\n",
    "        self.save_model('final_model.pth')\n",
    "        self.run.finish()\n",
    "    \n",
    "    def save_model(self, filename: str):\n",
    "        \"\"\"Save model with config and metrics\"\"\"\n",
    "        save_path = os.path.join(self.save_dir, filename)\n",
    "        torch.save({\n",
    "            'model_state_dict': self.model.state_dict(),\n",
    "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "            'config': self.config,\n",
    "            'best_val_acc': self.best_val_acc,\n",
    "            'best_val_loss': self.best_val_loss\n",
    "        }, save_path)\n",
    "        wandb.save(save_path)\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Configuration\n",
    "    config = {\n",
    "        'project_name': 'image_classification',\n",
    "        'model_name': 'EnhancedXRayClassifier',\n",
    "        'learning_rate': 0.0001,\n",
    "        'weight_decay': 1e-4,\n",
    "        'batch_size': 30,\n",
    "        'early_stopping_patience': 8,\n",
    "        'save_dir': './models',\n",
    "        'num_epochs': 100\n",
    "    }\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = CustomDataset(X_train, y_train)\n",
    "    val_dataset = CustomDataset(X_val, y_val)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    # Initialize model and trainer\n",
    "    model = EfficientMedicalNet(num_classes=5)  # Your model class\n",
    "    class_weights = [1 / count for count in label_list]\n",
    "    class_weights = torch.FloatTensor(class_weights)\n",
    "    device = torch.device(\"cuda\")\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights).to(device)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        config=config,\n",
    "        criterion=criterion,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader\n",
    "    )\n",
    "\n",
    "    # Train model\n",
    "    trainer.train(epochs=config['num_epochs'])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test dataset and loader\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=1\n",
    ")\n",
    "model = EfficientMedicalNet(num_classes=5)\n",
    "# Load best model\n",
    "checkpoint = torch.load('./models/best_model.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Evaluate\n",
    "report_df, confusion_matrix = evaluate_model(\n",
    "    model,\n",
    "    test_loader,\n",
    "    save_path='./results'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import math\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "class EnhancedModelVisualizer:\n",
    "    def __init__(self, model: nn.Module):\n",
    "        self.model = model\n",
    "        self.activation_maps = {}\n",
    "        self.attention_maps = {}\n",
    "        self.layer_stats = {}\n",
    "        self.hooks = []\n",
    "        self._register_hooks()\n",
    "    \n",
    "    def _activation_hook(self, name: str):\n",
    "        def hook(module, input, output):\n",
    "            self.activation_maps[name] = output.detach().cpu()\n",
    "            # Calculate statistical measures\n",
    "            with torch.no_grad():\n",
    "                stats = {\n",
    "                    'mean': float(output.mean()),\n",
    "                    'std': float(output.std()),\n",
    "                    'min': float(output.min()),\n",
    "                    'max': float(output.max()),\n",
    "                    'sparsity': float((output == 0).float().mean()),\n",
    "                    'shape': tuple(output.shape)\n",
    "                }\n",
    "                self.layer_stats[name] = stats\n",
    "        return hook\n",
    "\n",
    "    def _attention_hook(self, name: str):\n",
    "        def hook(module, input, output):\n",
    "            if isinstance(module, SEBlock):\n",
    "                # Store attention weights\n",
    "                self.attention_maps[name] = output.detach().cpu()\n",
    "        return hook\n",
    "    \n",
    "    def _register_hooks(self):\n",
    "        # Main component hooks\n",
    "        components = [\n",
    "            ('initial_conv', self.model.initial_conv),\n",
    "            ('stage1', self.model.stage1),\n",
    "            ('stage2', self.model.stage2),\n",
    "            ('stage3', self.model.stage3),\n",
    "            ('stage4', self.model.stage4),\n",
    "            ('fusion', self.model.fusion_conv)\n",
    "        ]\n",
    "        \n",
    "        for name, component in components:\n",
    "            self.hooks.append(component.register_forward_hook(\n",
    "                self._activation_hook(name)))\n",
    "        \n",
    "        # SE Block hooks\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, SEBlock):\n",
    "                self.hooks.append(module.register_forward_hook(\n",
    "                    self._attention_hook(name)))\n",
    "    \n",
    "    def remove_hooks(self):\n",
    "        for hook in self.hooks:\n",
    "            hook.remove()\n",
    "\n",
    "    def _plot_feature_maps_grid(self, feature_maps: torch.Tensor, \n",
    "                              title: str, max_channels: int = 16):\n",
    "        \"\"\"Plot multiple feature maps in a grid\"\"\"\n",
    "        # Select a subset of channels\n",
    "        n_channels = min(feature_maps.shape[0], max_channels)\n",
    "        feature_maps = feature_maps[:n_channels]\n",
    "        \n",
    "        # Calculate grid dimensions\n",
    "        grid_size = math.ceil(math.sqrt(n_channels))\n",
    "        fig, axes = plt.subplots(grid_size, grid_size, \n",
    "                               figsize=(15, 15))\n",
    "        fig.suptitle(f'{title}\\nShape: {feature_maps.shape}', y=0.95)\n",
    "        \n",
    "        # Plot each feature map with different visualization styles\n",
    "        for idx, feature_map in enumerate(feature_maps):\n",
    "            if idx >= grid_size * grid_size:\n",
    "                break\n",
    "                \n",
    "            row, col = idx // grid_size, idx % grid_size\n",
    "            ax = axes[row, col] if grid_size > 1 else axes\n",
    "            \n",
    "            # Alternate between different visualization styles\n",
    "            if idx % 3 == 0:\n",
    "                # Standard heatmap\n",
    "                im = ax.imshow(feature_map, cmap='viridis')\n",
    "            elif idx % 3 == 1:\n",
    "                # Contour plot\n",
    "                X, Y = np.meshgrid(np.arange(feature_map.shape[1]),\n",
    "                                 np.arange(feature_map.shape[0]))\n",
    "                im = ax.contourf(X, Y, feature_map, levels=20, cmap='magma')\n",
    "            else:\n",
    "                # Edge detection style\n",
    "                im = ax.imshow(feature_map, cmap='RdGy')\n",
    "            \n",
    "            ax.axis('off')\n",
    "            ax.set_title(f'Channel {idx}')\n",
    "        \n",
    "        # Remove empty subplots\n",
    "        for idx in range(n_channels, grid_size * grid_size):\n",
    "            row, col = idx // grid_size, idx % grid_size\n",
    "            axes[row, col].remove() if grid_size > 1 else None\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "    def _plot_attention_maps(self, save_path: str = None):\n",
    "        \"\"\"Visualize attention maps from SE blocks\"\"\"\n",
    "        if not self.attention_maps:\n",
    "            return\n",
    "        \n",
    "        n_maps = len(self.attention_maps)\n",
    "        fig, axes = plt.subplots(1, n_maps, figsize=(5 * n_maps, 4))\n",
    "        if n_maps == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, (name, attention) in enumerate(self.attention_maps.items()):\n",
    "            attention = attention.squeeze()\n",
    "            sns.heatmap(attention.numpy().reshape(-1, 1),\n",
    "                       cmap='YlOrRd',\n",
    "                       ax=axes[idx],\n",
    "                       cbar_kws={'label': 'Attention Weight'})\n",
    "            axes[idx].set_title(f'SE Block: {name}')\n",
    "            axes[idx].set_xlabel('Channels')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path.replace('.png', '_attention.png'))\n",
    "        plt.show()\n",
    "\n",
    "    def _plot_layer_statistics(self):\n",
    "        \"\"\"Plot detailed statistics for each layer\"\"\"\n",
    "        stats_names = ['mean', 'std', 'min', 'max', 'sparsity']\n",
    "        n_layers = len(self.layer_stats)\n",
    "        \n",
    "        fig, axes = plt.subplots(len(stats_names), 1, \n",
    "                                figsize=(12, 4 * len(stats_names)))\n",
    "        \n",
    "        for idx, stat_name in enumerate(stats_names):\n",
    "            values = [stats[stat_name] \n",
    "                     for stats in self.layer_stats.values()]\n",
    "            axes[idx].bar(self.layer_stats.keys(), values)\n",
    "            axes[idx].set_title(f'{stat_name.capitalize()} across layers')\n",
    "            axes[idx].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def visualize_all(self, input_image: torch.Tensor, \n",
    "                     save_path: str = None):\n",
    "        \"\"\"\n",
    "        Comprehensive visualization of model's internal representations\n",
    "        \n",
    "        Args:\n",
    "            input_image: Input tensor of shape (1, C, H, W)\n",
    "            save_path: Optional path to save visualizations\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            _ = self.model(input_image)\n",
    "        \n",
    "        # 1. Feature Maps Visualization\n",
    "        stages = ['Input Image', 'Initial Conv', 'Stage 1', 'Stage 2',\n",
    "                 'Stage 3', 'Stage 4', 'Final Fusion']\n",
    "        \n",
    "        # Plot input image\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(input_image[0].squeeze(), cmap='gray')\n",
    "        plt.title(f'Input Image\\nShape: {input_image.shape}')\n",
    "        plt.axis('off')\n",
    "        if save_path:\n",
    "            plt.savefig(save_path.replace('.png', '_input.png'))\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot feature maps for each stage\n",
    "        feature_maps = [\n",
    "            self.activation_maps['initial_conv'][0],\n",
    "            self.activation_maps['stage1'][0],\n",
    "            self.activation_maps['stage2'][0],\n",
    "            self.activation_maps['stage3'][0],\n",
    "            self.activation_maps['stage4'][0],\n",
    "            self.activation_maps['fusion'][0]\n",
    "        ]\n",
    "        \n",
    "        for feature_map, title in zip(feature_maps, stages[1:]):\n",
    "            fig = self._plot_feature_maps_grid(feature_map, title)\n",
    "            if save_path:\n",
    "                fig.savefig(save_path.replace('.png', \n",
    "                                            f'_{title.lower().replace(\" \", \"_\")}.png'))\n",
    "            plt.show()\n",
    "        \n",
    "        # 2. Attention Maps Visualization\n",
    "        self._plot_attention_maps(save_path)\n",
    "        \n",
    "        # 3. Layer Statistics Visualization\n",
    "        self._plot_layer_statistics()\n",
    "        \n",
    "        # 4. Print detailed layer information\n",
    "        self.print_detailed_summary(input_image)\n",
    "\n",
    "    def print_detailed_summary(self, input_image: torch.Tensor):\n",
    "        \"\"\"Print comprehensive summary of transformations and statistics\"\"\"\n",
    "        print(\"\\n=== Detailed Model Analysis ===\")\n",
    "        print(f\"\\nInput Image Shape: {input_image.shape}\")\n",
    "        \n",
    "        for name, stats in self.layer_stats.items():\n",
    "            print(f\"\\n{name.upper()} LAYER:\")\n",
    "            print(f\"Output Shape: {stats['shape']}\")\n",
    "            print(f\"Spatial Reduction: {input_image.shape[-1] / stats['shape'][-1]:.1f}x\")\n",
    "            print(f\"Number of Channels: {stats['shape'][1]}\")\n",
    "            print(\"\\nStatistics:\")\n",
    "            print(f\"  Mean Activation: {stats['mean']:.4f}\")\n",
    "            print(f\"  Std Deviation: {stats['std']:.4f}\")\n",
    "            print(f\"  Min/Max Values: {stats['min']:.4f} / {stats['max']:.4f}\")\n",
    "            print(f\"  Sparsity (zero activations): {stats['sparsity']*100:.1f}%\")\n",
    "            \n",
    "            if name in self.attention_maps:\n",
    "                attention = self.attention_maps[name]\n",
    "                print(\"\\nAttention Statistics:\")\n",
    "                print(f\"  Mean Attention Weight: {attention.mean():.4f}\")\n",
    "                print(f\"  Max Attention Channel: {attention.argmax().item()}\")\n",
    "\n",
    "def visualize_model_transformations(model_path: str, image_path: str):\n",
    "    \"\"\"\n",
    "    Visualize transformations for a single image through the model\n",
    "    \n",
    "    Args:\n",
    "        model_path: Path to the saved model\n",
    "        image_path: Path to the input image\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = EfficientMedicalNet(num_classes=5, in_channels=1)\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    from PIL import Image\n",
    "    import torchvision.transforms as transforms\n",
    "    \n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((512, 512)),\n",
    "        transforms.Grayscale(),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = preprocess(image).unsqueeze(0)\n",
    "    \n",
    "    # Create visualizer and generate visualizations\n",
    "    visualizer = EnhancedModelVisualizer(model)\n",
    "    \n",
    "    # Generate comprehensive visualizations\n",
    "    visualizer.visualize_all(\n",
    "        input_tensor,\n",
    "        save_path='model_analysis.png'\n",
    "    )\n",
    "    \n",
    "    # Clean up\n",
    "    visualizer.remove_hooks()\n",
    "\n",
    "# Example usage:\n",
    "visualize_model_transformations('./models/best_model.pth', 'DATASETS/kneeKL224/auto_test/0/9003175_1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
